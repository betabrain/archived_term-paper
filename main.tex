\documentclass[a4paper,man,12pt,apacite]{apa6} % man => jou
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\begin{document}

\title{Random Forests Destructured: Introduction, Overview, Possibilities}
\shorttitle{Random Forests Destructured}
\author{Tobias Ammann}
\keywords{ensemble methods, introduction, random forests}
\affiliation{Literature Study at the Workgroup for Psychological Methods, Evaluation and Statistics, Department of Psychology.\\Supervised by Prof. Dr. Carolin Strobl}

\abstract{Random forest are a machine learning technique that is getting some attention in psychological research lately. This paper will give an introduction to random forests and discuss some vital points.}
\keywords{ensemble methods, introduction, random forests}

\maketitle

\tableofcontents

\section{Introduction}

\subsection{Motivation}
Modern psychological research has to provide more than just good theories
and interesting case studies, because modern psychological research is
scientific. It has to follow the scientific method, which is to
provide good data and show that this data supports the assumption, hence
making it more likely to be the true. No theory is ever true, firstly,
because it wouldn't be called a theory any longer, and secondly, because
positive proof is impossible unless we are omniscient
(which we obviously are not).

Basically, the scientific method is the series of continuous attempts to
disprove alternative possibilities, until only a single theory remains.
Since this is an unweildly number of theories to disprove, and every
researcher likes to see the result of his work during his lifetime, a
more speedy method is employed, although this cheat comes with a caveat.
This speedier method has the researcher pit his favored theory against
the null hypothesis, a fancy word for chance. This certainly is
way more efficient. The caveat, commonly called confirmation bias, is that
the result only has significance in the experimental setup being tested.
On the greater scale of things, i.e. in the field or reality, the resuls
might well be completely bogus.

However, many of the things being studied are constant or change
predictably enough to allow a generalization from the experiment to the world
that is likely to be true. This likelihood depends on the size of the
effect in the experiment, the number of times it was observed, and on
the properties of the test involved.

In psychological research, where large effects are freakishly rare and
experiments usually study only a handful of psychology students, it is
vital to use good statistical methods, because that is the only parameter
remaining for the researcher to tweak in his favour.
Recently researchers in psychology began to turn to a new breed of
statistical methods, in hope of ever better results. This new breed of
statistical methods is called machine learning and has it's origins in
artificial intelligence.

This report will discuss a rather new machine learning algorithm called
\emph{random forest}, it's qualities, and a brush over a small number of
improvements that have been tried.

\section{Random Forests}

\subsection{Machine Learning}

\subsubsection{Life Cycle}
Here I'll discuss the difference between machine learning and traditional
statistical methods. Terminology.

\subsubsection{Classification}
The classification problem. Terminology.

\subsubsection{Regression}
A short discussion of regression, the why and how, and the difference
between classical regression and regression in machine learning.
Terminology.

\subsubsection{Randomness}
Warn about fluctuations in modell outputs. Seeds.

\subsection{Ensembles}
A short introduction to ensembles and their properties. Terminology.
Weak and strong learners.

\subsubsection{Bootstrapping}
Sampling the data.

\subsubsection{Random Subspaces}
Sampling the variables.

\subsection{Classifiers}

\subsubsection{Decision Trees}
This is the basis.

\subsubsection{Fuzzy Trees}
...but trees can be more than simple trees.

\subsection{Putting them together}

\subsubsection{Random Forests}
Breiman

\subsubsection{Alternative: Dynamic Random Forests}
Boosting random forests.

\subsubsection{Alternative: Rotation Forest}
Massaging features before growing trees.

\subsubsection{Parameters and Biases}

\subsubsection{Alternative: PERT}
Random variables and random splits.

\section{Method}

\subsection{Selection of Papers}
I have started my research on \emph{random forests} by reading the
introductory paper suggested by my supervisor titled
\emph{An introduction to recursive partitioning: Rationale, application
and characteristics of classification and regression trees, bagging and
random forests \cite{strobl2009introduction}}. I then searched
the research databases \emph{PsychARTICLES} and \emph{PsychINFO}
querying for \emph{random forest} and limiting my results to the last
two years.
I only considered papers that discuss \emph{random forests} and dismissed
every paper where \emph{random forests} are merely used as a research
method. I also used \emph{Google Scholar} to look for more technical
publications outside the field of psychology. I searched for combinations
of \emph{random forest}, \emph{comparison}, \emph{analysis},
and \emph{ensemble}. I also included keywords seen in interesting titles,
like \emph{fuzzy}, \emph{perfect}, \emph{full}, \emph{balanced},
\emph{extremely}, and \emph{rotation}.
I also queried for some of the referenced publications while I read the
found material, but only included \cite{strobl2008conditional}, because
it was referenced multiple times, and co-authored by my supervisor.

The final criteria for inclusion were the online availability of a freely
downloadable PDF-file, which often extremely easy thanks to
\emph{Google Scholar}'s simple interface, and my decision on the topic of
this report.

\section{Conclusion}
State of the art? Not really. They are not yet properly understood, and
many comparisons and lots of the advantages might be accidential.
Strong dependance on parameters, with no rationally pleasing way to set them.

\subsection{The End}
Bye.

\bibliography{references}

\end{document}

