\documentclass[a4paper,man,12pt,apacite,floatsintext,draftfirst]{apa6} % man => jou
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{mathtools}
\usepackage{url}
\usepackage{tikz}
\usepackage{listings}
\usepackage{framed}
\usepackage{etoolbox}

\BeforeBeginEnvironment{verbatim}{\def\baselinestretch{1}\noindent}

\tikzset{
  treenode/.style = {align=center, inner sep=2pt, text centered,
    font=\sffamily, minimum size=0.5cm},
  arn_c/.style = {treenode, rectangle, draw=black},
  arn_d/.style = {treenode, ellipse, draw=black}
}

\title{Random Forests: Introduction, Parts, Variants}
\shorttitle{Random Forests: Introduction, Parts, Variants}
\author{Tobias Ammann}
\authornote{Tobias Ammann\\Alte Landstrasse 39\\8802 Kilchberg\\tag@adnm.ch}
\keywords{ensemble methods, introduction, random forests}
\affiliation{Literature Study at the Workgroup for Psychological Methods,\\Evaluation and Statistics, Department of Psychology.\\Supervised by Prof. Dr. Carolin Strobl}
\note{DRAFT Version: \today}
\keywords{ensemble methods, introduction, random forests}

\abstract{This report looks at a machine learning algorithms called
random forests, their structure, problems, and a number of suggested
improvements, to give the reader a more big-picture introduction.
The topics discussed are the origins of machine learning, machine learning
vs traditional statistics, ensemble learning, bagging, boosting, 
decision trees, random forests, parameters, out-of-bag data, error estimates,
variable importance, regression, overfitting, optimality criterion,
the random forest variants PERT, fuzzy random forests, and rotation forests.
This report puts forward a few ideas on how to make random forests better
and more user-friendly.}

\begin{document}
\maketitle

\tableofcontents

\newpage
\section{Introduction}

\subsection{Motivation}
Psychology has become a science, thus psychological research has to follow
the \emph{scientific method}, according to which positive proof is an
impossibility unless we have complete knowledge, and could eliminate all
alternative theories.
However, we won't ever have complete knowledge, therefore scientific isn't
about proofs, but probabilities.
Research works under the assumption that if we disprove just enough
alternative theories, we can eventually tell which theory is probably
true.
So, the scientific method really is nothing but the use of countless
attempts to disprove alternative theories, until only a single such theory
remains.

Since there is an unweildly number of theories to disprove, and every
researcher likes to see the result of his work during his lifetime, a
more speedy method is usually employed, although this comes with a caveat.
The speedier method has the researcher pit his favored theory against
the null hypothesis, a fancy word for chance.
This is way more efficient than comparing the thousands of
theories that researchers have come up with, and continue to come up with.
The caveat, commonly called confirmation bias, is that the result only
has significance in the experimental set-up being tested.
In the greater scale of things, i.e. reality, the results might well be
completely bogus.
Nonetheless, most of the subjects being studied are sufficiently constant
or change predictably enough to allow researchers to generalize from the
results of an experiment to the world at large, and likely remain correct.
This likelihood depends on the size of the effects measured in the
experiment, the number of experimental subjects, and on the properties of the
statistical methods involved.
In psychological research, where large effects are rare and
experiments usually study only a handful of psychology students, it is
vital to use good statistical methods, because that is the only parameter
remaining for the researcher to tweak in his favour.

Recently, researchers in psychology began to turn to a new breed of
statistical methods, in hope of ever better results. This new breed of
statistical methods is called \emph{machine learning}.

In this paper, I aim to introduce the reader to \emph{random forests},
which are just one family of algorithms.
I intend to do this in a way that gives every reader a chance to understand
this method without prior knowledge. I also intend to present the reader
with some context around random forests, in hope that they will benefit
from a more big-picture view.

In the next sections I introduce the reader to machine learning,
the differences between the traditional statistical and this new
machine learning approach, random forests in particular, and finally delve
into some improvements to random forests that have been suggested in the
literature.

\subsection{Machine Learning}
In order to understand random forests, it might be useful to set
the stage by briefly discussing machine learning in general.
Machine Learning is both a part of predictive statistics and the
artificial intelligence branch of computer science.

\emph{Predictive statistics} is the sub-field of statistics that is
concerned with making predictions based on past observations.
It's probably most widely known method is \emph{linear regression}
\cite{wpLR} that associates two variables \(y\) and \(x\) in such a way
that they describe a straight line: \(y = \alpha * x + \beta \).
Predictive statistics is widely used in psychology because it
allows the researcher to look at the unobservable by making
assumptions of the form \(reaction = mind * stimulus + variation\).
This is the standard approach in personality questionaires.

\emph{Artificial intelligence} is a field commonly associated with the
computer sciences, where it began with the advent of higher-order
programming languages based on mathematical foundations around the \emph{1960s} \cite{wpHOPL}.
Its aim is to give computers human-like capabilites, so that they can assist
us by combining intelligence, with flawless logic and super-human knowledge.
It includes things like \emph{logic programming} \cite{wpLP},
\emph{expert systems} \cite{wpES}, \emph{databases} \cite{wpDB} and
\emph{neural networks} \cite{wpNN}, that all represent some form of
storing and querying knowledge.
Unfortunately, early computers back then didn't have the speed and memory
required to push the envelope far enough, and the field was deemed dead.
Only the rather recent coexistance of powerful computers and massive amounts
of stored data, sometimes called \emph{big data}, revived artificial
intelligence as an important field of research.

\emph{Machine learning} is that part of artificial intelligence that
is concerned with the computer's learning of facts about the world.
These facts can then be stored and subsequently queried later on.
As such machine learning is concerned with making statements based
on past observations, and, is therefore, close to predictive statistics
\cite{wpML}.

\subsection{The Machine Learning Life Cycle}
This section discusses the difference between machine learning and traditional
statistical methods. Terminology.
The following section heavily relies on information found in Wikipedia, as
well as what I learned in statistics lectures in the past years.
A good introductory article is \cite{wpML}.

Traditionally, the statistical methods used in psychology
take a model of how the world works, and a set of data, and return one of
two things.
They either return a probability of how likely an improvement in prediction
can be observed at random, or how likely a difference in measurement can be
observed at random.

\emph{Regression methods} try to derive the values of
one variable from the other variables in the dataset using a formula the
researcher specifies.
They then compare the actual values and the prediction my the model with
different inputs, and calculate how probable an improvement in this
comparison is to show up due to random variations \cite{wpRA}.

\emph{Analysis of variance methods} partition the dataset
according to all but one variable. They then calculate the probability with
which the variation in the one variable among the groups could be due to
random variations in the dataset \cite{wpAOV}.

The probabilities the methods output are what statisticians call the
significance. Statisticians usually define a target significance level,
e.g. 5\%, and compare it to the output of their statistical calculations.
If the calculated probability is less than the targeted significance level the
measured effects are said to he significant at the chosen significance level.
For example, a result that is significant at 5\%, we know that it is less
likely to show up at random than in 5\% of all experiments.

The most striking difference between traditional statistical methods and
machine learning methods is that the researcher can't specify his model of
how the world works, other than through the selection of a machine learning
algorithm.
Because of this, machine learning algorithms are sometimes described as
\emph{black boxes}, meaning that the user can only see what's going into the
algorithm, and what is coming out.
This is unlike the statistical methods, where the researcher supplies a
formula, because in machine learning, algorithms derive the model on their own.
This is what the learning in machine learning means.
The second difference is what the algorithms return.
Since machine learning algorithms represent the model, what they output is
not a percentage, i.e. significance, but the model itself.
In short, machine learning provides the researcher with a generic model
that adapts to the world.
The prediction of such a model can then be calculated for data for which
the values of the output variable are known, but that hasn't
been included in the learning phase, to calculate the significance.
The problem with this flexibility is, that one cannot really tell what the
model looks like, that is, the model is not in a human-readable form.
As will be pointed out later, decision trees, the underlying mechanism in
random forests, are quite easily understandable, but random forests
consist of dozens to thousands of such trees, so that a human can hardly
tell what they mean.
Therefore, it is very important to find ways to condense this complexity
into something that can be more easily interpreted.
The variable importance measure of random forests, which will be introduced
later, is one such way.

\subsection{Classification}
The \emph{classification problem} is the problem of classifying new data based
on a set of example data, but without the explicit set of rules that guided the
classification of the example data.
Unlike in \emph{regression}, the result here is one of many given classes
and not a numeric value.
One might describe classification as regression with discreet output values.
Output variables are commonly called \emph{classes}, while input variables
are commonly called \emph{features} independent of the type of problem.

\subsection{Regression}
A short discussion of regression, the why and how, and the difference
between classical regression and regression in machine learning.
Terminology. Classification with continuous classes.

\subsection{Randomness}
Many machine learning algorithms consume random numbers in different places.
While computer generated random numbers are not truly random, they still
cause the outcome of the algorithm to change slightly between different
executions, due to the fact that they are customarily initialized with
the current time at the start of the program.
These changes might unnerve a novice, but don't change the outcome of the
algorithm significantly.
Still, for publication purposes it can make sense to set and publish the
random number generator's \emph{seed}, i.e. the value the generator is being
initialized with.
However, doing so during the experiment
is a serious mistake.
Indeed, it is good practice to run the analysis multiple times to ensure
that these random variations don't change the outcome.

\newpage
\section{Random Forests}

\subsection{Introduction}
This part of the paper discusses random forests.
“Random forests are a combination of tree predictors such that each tree
depends on the values of a random vector sampled independently and with
the same distribution for all trees in the forest” \cite{breiman2001random}.
Leo Breiman developed random forests with Adele Cutler, building on work
by Ho, Amit, Geman, and Dietterich \cite{wpRF}.

Although the name random forests is usually taken to refer to the random
forests as defined by \cite{breiman2001random}, the large number of
variants that have been derived from the original forests, e.g.
Forest-rk \cite{bernard2008forest}, RFW \cite{maudes2012random},
DRF \cite{bernard2012dynamic}, Fuzzy random forests \cite{bonissone2008fuzzy},
Rotation forest \cite{rodriguez2006rotation}, random forests that are
more random \cite{geurts2006extremely}, \cite{liu2005maximizing},
\cite{cutler2001pert}, and various other improvements, e.g. by
\cite{banfield2007comparison}, \cite{robnik2004improving},
\cite{strobl2009introduction}, \cite{zhang2012bias}, make it so that it
is better to think of random forests as being a framework instead of being
a single method \cite{wpRF}.
To understand this framework, it is best to look at the different aspects
of random forests, first in a top-down view, and later part by part.
The top-down view is strictly based on \cite{breiman2001random},
while the part by part discussion will also go into tweaking random forests.

Random forests are ensemble learning methods where the ensemble consists
of decision trees.
Every decision tree is constructed on a sample of the input dataset,
that has been selected using bootstrapping with replacement from the original
dataset and is equally large.
Every node split in the decision tree is an optimal two-way split selected
from a random subset of all input variables.
The number of randomly selected variables for each split is commonly called
\texttt{mtry}.
If the number of input variables is small, additional input variables can be
derived as linear combinations of input variables.
The decision trees are grown maximally without pruning,
and new trees are generated until the ensemble of decision trees reaches its
target size, usually called \texttt{ntree}.
Random forests feature error estimates using out-of-bag data.
Out-of-bag data are the records in the dataset that were not selected during
the bootstrap aggregation, and make up approximately one third of the dataset.
Random forests also feature variable importance measures,
which are calculated by reclassifying the out-of-bag data,
but randomizing the variable under consideration.
The variable importance of the randomized variable is the increase of
misclassifications.

The reference implementation of random forests is written in Fortran,
but a package for the statistical software framework R \cite{rproject2012},
exists under the name \texttt{randomForest} \cite{liaw2002classification}.
An alternative within the R framework, which includes improvements
to correct a bias found in variable
importance measures is available under the name \texttt{party}
\cite{strobl2008conditional}.
A third implementation using Java is available in the WEKA machine learning
suite \cite{hall2009weka}.
For illustration purposes I include a source code example taken from
\cite{strobl2008conditional}:

\begin{framed}
\begin{verbatim}
> load("dat_smoking.rda")
> library("party")
> myctree <- ctree(intention_to_smoke ~ ., data = dat_smoking)
> class(dat_smoking$intention_to_smoke)
> plot(myctree)
\end{verbatim}
\end{framed}

The use case for random forests is quite wide.
Random forests have been used in applications from psychology and
computational biology as is outlined in \cite{strobl2009introduction},
to customer churn prediction \cite{xie2009customer},
to software testing \cite{guo2004robust} and internet security
\cite{zhang2005network}, to natural language applications \cite{xu2004random}
and \cite{kobylinski2008definition}.
The reasons why random forests are such a widely used method,
are their prediction accuracy, which is comparable to other state-of-the-art
machine learning algorithms like Adaboost as demonstrated
in \cite{breiman2001random},
their ability to handle “small n large p” datasets \cite{strobl2009introduction},
their practical built in error estimate, and their variable importance measures.
The last of which, random forests' variable importance measures,
might be its most useful feature.
Many domains don't require accurate predictions as much as a model that can
be understood by humans.
While ensemble methods are unsuitably complex,
random forests' variable importance measures can be used to select variables
for use in simpler models, e.g.
generalized linear models, logit and probit models,
which are more easily interpreted \cite{strobl2009introduction}.

\subsection{Ensemble Learning Methods}
As mentioned above, random forests build on quite a rich collection of
previous work, most of which also concerns ensemble learning methods.
In fact, random forests can be seen as a composition of elements from
different other ensemble learning methods, e.g.
random subspaces \cite{ho1998random} and bagging \cite{breiman1996bagging}.
Although detailed knowledge of these methods is not a requirement to
understand random forests, it is important to understand what
ensemble learning is.
Again, this paper will first give an abstract definition and then look at
examples in detail.

Ensemble learning is a supervised learning algorithm: its task is to take
an example of input and output data, and find a hypothesis that connects
the two.
This hypothesis can then be used to predict the output data that
corresponds to new input data.
In theory this problem can be solved by a construct called
Bayes optimal classifier, which considers every possible hypothesis,
but which unfortunately can't be implemented except for trivial problems.
However, the principle that the combination of possible hypotheses becomes
a stronger hypothesis, because the ensemble can represent more functions
than every component hypothesis could holds.
In short, ensemble methods rely on the principle that the ensemble is more
than the sum of its parts.
The usual wording of this is, that ensemble learning turns a set of
\emph{weak classifiers} into one \emph{strong classifier}.
Weak also stands for unstable, meaning that the underlying classifier is
susceptible to even small variations in the input data.
Ensemble learning methods can be called meta algorithms,
because they rely on other simpler classifier algorithms,
and it is theoretically possible to create an ensemble learner for any
supervised learning algorithm. \cite{wpEL} \cite{Polikar:2009}

\subsubsection{Bagging}

The ensemble learning algorithm that most prominently underlies
random forests is bagging. Bagging stands for bootstrap aggregation.
Bootstrapping is a term that was used to describe
“[...] the process by which lumberjacks hoist themselves up trees [...]”
\cite{wpBOOT}.
In statistics it refers to the process of deriving additional samples
by resampling the original sample,
essentially simulating drawing additional samples from the population.
Bootstrap aggregation is an ensemble learning algorithm,
which trains each of its underlying weak classifiers on a different set
of input data created by bootstrapping.
It is another algorithm developed by Leo Breiman \cite{breiman1996bagging}.

Bagging typically uses bootstrapping with replacement,
which leads to the inclusion of on average two thirds of the original sample
in the derived sample.
Random forests use the remaining third as out-of-bag data
\cite{breiman2001random}.

\subsubsection{Boosting}

Boosting is an ensemble method, where every weak classifiers gets to train
on the original sample, but the sample is improved with importance weights.
These weights are different for every instance of the weak classifier.
They are lower for records in the dataset that are correctly predicted by
the classifiers already in the ensemble,
and higher for records that are classified wrongly.
In short, boosting focuses on eliminating one classification error after
another, until all data is being classified correctly,
or a targeted ensemble size is reached.
This behavior makes boosting algorithms very fast learners,
but susceptible to errors in the dataset \cite{long2010random}.
Boosting isn't used in random forests as introduced by \cite{breiman2001random},
but there are variants who do, e.g.
dynamic random forests \cite{bernard2012dynamic}.

Adaboost \cite{freund1995decision} is probably the most popular
boosting algorithm, and the algorithm random forests are most commonly
compared to in terms of performance, e.g. \cite{breiman2001random},
\cite{banfield2007comparison} and \cite{rodriguez2006rotation}.
The latter compared eight ensemble of decision tree classifiers in a
large study on 57 publicly available datasets and concluded
“[...] that boosting, random forests and randomized trees are
statistically significantly better than bagging.”

\subsection{Decision Trees}

The underlying algorithm of random forests is the decision tree classifier,
which can be either a regression tree, or a classification tree.
The difference between the two types of trees is the type of output produced.
Classifications trees work with discreet output values,
while regression trees output a continuous numeric value.
For classification the most commonly chosen output value among an ensemble
of decision trees is assumed to be the right prediction.
For regression purposes the output of the ensemble is calculated by averaging
the outcomes the single trees.
As the name indicates, a decision tree is a tree data structure, i.e.
a set of nodes that are connected in a forward manner allowing branching
but not cross links and circles.
Decision trees usually are binary trees, meaning that each node either has
two child nodes, or is a leaf node.
Each node represents a decision criterion, each edge a criterion match or
mismatch, and each leaf node an outcome of the represented decision tree.
Tree data structures are typically drawn from top to bottom.
For example, the following decision tree takes three numeric features
\texttt{a}, \texttt{b}, \texttt{c} to predict a discreet output variable
\texttt{class} with four possible values.

\begin{framed}
\begin{center}
\begin{tikzpicture}[level/.style={sibling distance = 7cm/#1}]
\usetikzlibrary{shapes}
\usetikzlibrary{arrows}
\node [arn_d] {feature a}
    child{ node [arn_c] {class = w} edge from parent node[left] {\(a \le 4.56\)} }
    child{ node [arn_d] {feature b}
        child{ node [arn_d] {feature c}
            child{ node [arn_c] {class = x} edge from parent node[left] {\(c \le 9.11\)} }
            child{ node [arn_c] {class = y} edge from parent node[right] {\(d > 9.11\)} }
            edge from parent node[left] {\(b \le 3.1\)}
        }
        child{ node [arn_c] {class = z} edge from parent node[right] {\(c > 3.1\)} }
        edge from parent node[right] {\(a > 4.56\)}
    }

;
\end{tikzpicture}
\end{center}
\end{framed}

The decision trees used by random forests are constructed by randomly
selecting a certain number of input variables as configured by the parameter
\texttt{mtry} for each node.
The algorithm then selects the best threshold among these input
variables as the node's criterion, using what is called an optimality criterion.
This process is repeated for every child node until a node is reached
which only matches records in the dataset with the same output value.
This node becomes a leaf node.
This is described as growing a tree maximally.
It ensures that each decision tree has a high strength,
meaning no decision tree outputs wrong predictions for the subsets of data
it matches. \cite{breiman2001random}

As a component classifier to an ensemble learning algorithm,
every decision tree should be different from the other decision trees in
the ensemble, i.e. the decision trees should be diverse,
to cover a wide range of possible interactions between the input data,
i.e. hypotheses.
This is commonly described as the trees being uncorrelated.
Tree strength and inter-tree correlation are what the prediction accuracy
of random forests depends on. \cite{breiman2001random}

The procedure by which decision trees are grown is modified in some
variants of random forests.
The PERT algorithm \cite{cutler2001pert} for example doesn't perform a
search for the best split on a random selection of input variables,
but selects variable and threshold at random.
RFW \cite{maudes2012random} searches all variables but attaches random
weights to them, and DRF \cite{bernard2012dynamic} influences the
decision tree creation by a procedure inspired by boosting.
Other variants, e.g. \cite{van2012accelerating}, change this procedure to
grow smaller or more balanced trees, to meet performance or
hardware requirements.

\subsection{Random Forests}
As has been described in the sections on ensemble learning and decision trees,
random forests build ensembles of decision trees.
This process depends on two parameters: the ensemble size, commonly called
\texttt{ntree}, and the randomization parameter \texttt{mtry}.
Actually, random forests depend on another parameter,
the state of a random number generator, but being random,
it doesn't and usually shouldn't be specified.
The following is an exemplary implementation of random forest following the
original version by \cite{breiman2001random}, but with limited features and
with entropy instead of the gini index as optimality criterion.
In reference to the higher level programming languages mentioned
in the introduction, the following code uses a Lisp
dialect called Clojure \cite{wpCLOJURE}.

\begin{framed}
\begin{verbatim}
(ns rf.core
  (:require [clojure.zip :as zip]))

(def sample [
  {:a 0 :b 0 :c 0 :d 0 :class :0} {:a 1 :b 0 :c 0 :d 0 :class :8}
  {:a 0 :b 0 :c 0 :d 1 :class :1} {:a 1 :b 0 :c 0 :d 1 :class :9}
  {:a 0 :b 0 :c 1 :d 0 :class :2} {:a 1 :b 0 :c 1 :d 0 :class :a}
  {:a 0 :b 0 :c 1 :d 1 :class :3} {:a 1 :b 0 :c 1 :d 1 :class :b}
  {:a 0 :b 1 :c 0 :d 0 :class :4} {:a 1 :b 1 :c 0 :d 0 :class :c}
  {:a 0 :b 1 :c 0 :d 1 :class :5} {:a 1 :b 1 :c 0 :d 1 :class :d}
  {:a 0 :b 1 :c 1 :d 0 :class :6} {:a 1 :b 1 :c 1 :d 0 :class :e}
  {:a 0 :b 1 :c 1 :d 1 :class :7} {:a 1 :b 1 :c 1 :d 1 :class :f}])

(defn bootstrap [dataset]
  (let [n (count dataset)
        s (repeatedly n #(rand-int n))
        d (set (map #(nth dataset %) s))
        o (set (filter (comp not d) dataset))]
    {:dataset d :oob o}))

(defn select [[v s] dataset]
  (let [l (reduce (fn [acc t]
                    (if (<= (get t v) s) (conj acc t) acc))
            #{} dataset)
        r (set (filter (comp not l) dataset))]
    [l r]))

(defn entropy [dataset]
  (let [n (count dataset)
        p (map #(/ % n) (vals (frequencies (map :class dataset))))
        l #(/ (Math/log %) (Math/log 2))]
    (* -1 (reduce #(+ %1 (* %2 (l %2))) 0.0 p))))

(defn splits [dataset mtry]
  (let [vs (filter (partial not= :class) (keys (first dataset)))
        n (count vs)
        vs (if (<= mtry n) (take mtry (shuffle vs)) vs)]
    (reduce (fn [acc v]
              (reduce (fn [acc t]
                        (conj acc [v (get t v)])) acc dataset))
            #{} vs)))

(defn best-split [dataset oob mtry]
  (let [[_ s l r]
    (first (sort (map (fn [s] (let [[l r] (select s dataset)]
                                [(entropy l) s l r]))
                      (splits dataset mtry))))]
    (let [[oobl oobr] (select s oob)]
      [s (set l) (set r) (set oobl) (set oobr)])))

(defn leaf? [dataset] (= (entropy dataset) 0.0))

(defn extend-node [{:keys [dataset oob] :as node} mtry]
  (if (leaf? dataset)
    (merge node
           {:class (or (first (first (sort-by val >
                         (frequencies (map :class dataset)))))
                       :unknown)})
    (let [[s l r oobl oobr] (best-split dataset oob mtry)]
      (merge node
             {:criterion s :left  {:dataset l :oob oobl}
                           :right {:dataset r :oob oobr}}))))

(defn rf-train [dataset ntree mtry]
  (set (pmap (fn [sample]
         (loop [loc (zip/zipper
                      (fn [node] true)
                      #(if (:left %)
                        (seq [(:left %) (:right %)]))
                      (fn [node children]
                        (with-meta
                          (merge node
                                 {:left  (first children)
                                  :right (second children)})
                          (meta node)))
                      sample)]
           (if (zip/end? loc) (zip/root loc)
             (recur
               (zip/next (zip/edit loc extend-node mtry)))))
         (repeatedly ntree #(bootstrap dataset)))))

(defn rf-predict [forest features]
  (let [eval-tree
          (fn eval-tree [tree]
            (if (:class tree) (:class tree)
              (if (let [[v s] (:criterion tree)]
                    (<= (get features v) s))
                  (eval-tree (:left tree))
                  (eval-tree (:right tree)))))]
    (first (first
      (sort-by
        val > (frequencies
                (filter (partial not= :unknown)
                        (pmap eval-tree forest))))))))

(defn tree-error [tree]
  (let [cls (:class tree)]
    (cond (nil? cls) (+ (tree-error (:left tree))
                        (tree-error (:right tree)))
          (= cls :unknown) 0
          :else (count
                  (filter (partial = cls) (:oob tree))))))

(defn rf-error [forest]
  (/ (reduce + 0
       (pmap #(/ (tree-error %) (count (:oob %))) forest))
     (count forest)))

(def rf (rf-train sample 1 3))

(every? #(= (rf-predict rf (dissoc % :class)) (:class %))
        sample)

(rf-error rf)

\end{verbatim}
\end{framed}

\subsubsection{Parameters}
The two parameters random forests depends on, the size of the forest
\texttt{ntree} and the number of randomly selected variables at each node
\texttt{mtry}, are problematic,
in that users set them according to one recommendation or another,
or just use the default settings, which again can differ from implementation
to implementation.
Furthermore, not every dataset is best learned by random forests using the
same settings.
This lack of a standard way to determine the parameter values makes it hard
to compare the accuracy of different methods.
The comparative study by \cite{banfield2007comparison} gives a nice overview
over the datasets used in previous publications,
among which are \cite{breiman2001random} and \cite{dietterich2000ensemble},
that each used ensemble sizes of 50-200, and applied the algorithms being
compared to 18-27 datasets.
In each case the authors concluded that their method was superior to the
other methods in the study.
However, the more recent study by \cite{banfield2007comparison} found
“no stat. sig. improvement over bagging in 38 of 57 data sets”
when ensemble sizes of up to 1000 trees where used.
The study also concludes that bagging with an ensemble size of 1000 and
random forests with the randomization parameter set to the
binary logarithm of the number of input variables were the best methods.
The study by \cite{banfield2007comparison} also suggests a mechanism
to determine the best size of the ensemble automatically.
Methodologically, this would be a welcome improvement, because it takes
away one way researchers can fiddle with the outcome of their calculations,
and because it would ensure better results due to the usually larger
ensemble sizes.
In defense of older studies, like those by \cite{breiman2001random} and
\cite{dietterich2000ensemble}, one has to mention that
computers back then weren't as powerful.
\cite{breiman2001random} mentions run times for random forests of 4 minutes
and 3 hours for Adaboost when building ensembles of size 100,
while I can execute all examples in \cite{strobl2009introduction}
in well under 30 seconds.

The second parameter of random forests is the randomization parameter
\texttt{mtry} which controls how many variables are being selected randomly
at each decision tree node to search for an optimal split.
If \texttt{mtry} is set to 1, random forests acts like a linear combination
of the input variables.
If \texttt{mtry} is set to the number of variables, random forest becomes
bagging as proposed by \cite{breiman1996bagging}.
Common choices for \texttt{mtry} are 1, 2, and other small values in
older studies, e.g. \cite{breiman2001random}, and the square root of n,
or the binary logarithm of n, with n being the number of input variables,
becoming increasingly popular in newer literature, e.g.
\cite{strobl2009introduction}.
As mentioned above, using the binary logarithm of the number of
input variables is indeed the best choice for most datasets
\cite{banfield2007comparison}.
However, depending on the dataset and the ensemble size, \texttt{mtry}
needs to be chosen differently.
For example in studies of genetics, where the dataset often includes many
irrelevant input variables in addition to the dataset being a
“small n large p case”, it is necessary to use a larger value
for \texttt{mtry}, else some variables might never be used in the ensemble
at all \cite{strobl2009introduction}.
It might be interesting to consider choosing \texttt{mtry} automatically,
and two obvious suggestions for this would be to choose a random \texttt{mtry}
value for every decision tree being grown, or to define it via a function of the
number of input variables satisfying some statistical distribution criterion.
According to \cite{bernard2008forest} a greedy search or choosing one of
the values discussed above is the usual approach in the literature.
The same paper shows, that this doesn't have to be the case, by demonstrating
a "push-button" method that automatically derives a suitable value for
\texttt{mtry} and "is at least as statistically significant as the original".

\subsubsection{Out-of-bag Data}
Because random forests are based on bagging and use bootstrapped samples,
each tree has a set of approximately one third of the original dataset
that has not been used to grow the tree.
This set can be used to test the prediction accuracy of each tree,
and the prediction accuracy of all trees can then be averaged to give an
error estimation of the entire random forest.
This out-of-bag error estimation is more precise than the standard error
estimate, which is using all of the dataset \cite{strobl2009introduction}.
This said, one should not forget that out-of-bag data isn't the
same thing as a genuine test dataset.
Depending on the size of the dataset, and the size of the ensemble,
the downsides
of bootstrapping might shine through, and lead to an underestimation of
the prediction accuracy.

\subsubsection{Variable Importance}
As has been mentioned above, random forests have a built in variable
importance measure, which is calculated, by permutating one input variable
in the out-of-bag data of every tree, and calculating a new error estimate.
The difference between the out-of-bag error estimates with and without
randomly permutated input variable is the variable importance.
The more important a variable is, the more drastically the prediction error
increases when the variable is being randomized.
The variable importance measure is sometimes scaled, i.e. z-standardized,
but because it strongly depends on the parameter of random forests,
it's not possible to compare these variable importances across studies,
hence there is little use in doing so, and the practice only encourages
problematic comparisons across studies \cite{strobl2009introduction}.

The idea behind this way of calculating variable importances is,
that one would like to compare a prediction model with and without a
particular input variable to measure the variable's impact.
Obviously, one can't ignore all trees that incorporate a variable,
because each tree incorporates multiple input variables, and their impact
on the prediction would be modified too.
By randomly permutating the values of an input variable,
the variable's characteristics don't change, but the connection to the
output variable is broken.
However, according to \cite{strobl2007bias} the variable importance might
still depend on which variable is being measured, because the
variable importance measure is biased towards variables with many categories
and variables with many missing values.
Numeric variable usually have as many different values as there are records
in the dataset, meaning that their importance measure is greatly biased due
to the large number of “categories”.
Fortunately, this can be fixed, but “Only when subsamples drawn without
replacement, instead of bootstrap samples, in combination with unbiased
split selection criteria, are used in constructing the forest, can the
resulting permutation importance be interpreted reliably”
\cite{strobl2007bias}, and correlated input variables still are
problematic.
The R package \texttt{party} includes two functions, \texttt{ctree} and
\texttt{cforest}, that are not affected by this bias, due to their use
yet another variable importance termed “conditional variable importance”
\cite{strobl2008conditional}.
Correlated variables can be problematic, because trees that include a
pair of correlated input variables are less affected by the random
permutation of one of them.
Conditional variable importance considers these correlations.
However, \cite{gromping2009variable} argues, that this problem can't be avoided
as long as \texttt{mtry} is smaller than the number of input variables,
and that considering all input variables, i.e. bagging,
“might already go a long way” towards remedying the problem,
although due to the “large p small n case”, “unbiased estimation of all
coefficients is impossible” in any case.

Another problem that the variable importance measure can be affected by,
is that some variables might not be well represented in a random forest.
This can be due to a small setting for \texttt{mtry} and or \texttt{ntree}
in the presence of many irrelevant input variables, as often is the case
in genetics datasets, or if variables show perfect higher order effects,
i.e. interaction effects, but no main effects.
The latter is called the XOR problem.
It is important to note, that random forests with a different split selection
algorithm don't have to be affected by this, e.g. PERT \cite{cutler2001pert}.

Last but no least, both \cite{strobl2009introduction} and
\cite{gromping2009variable} see one of the advantages of random forests in
their variable importance measure.
\cite{gromping2009variable} compares linear variable importance measures
with the one built into random forests, and finds that the latter are
heavily dependent on the mtry parameter.
The larger \texttt{mtry} is, the better the importance estimates become.
Variable importance measures are used to select input variables for a simpler
model, e.g. a generalized liner model, which is more interpretable than a
forest of decision trees \cite{strobl2009introduction}.
Variable importance measures also are the probably only simple way to
“shed some light into the black box of random forests”
\cite{gromping2009variable}.
A alternative, but rather naive way to estimate variable importance is to count
the occurance of a variable over all trees \cite{strobl2009introduction}.
Other imaginable ways to estimate variable importance would be to use
algorithms that analyze the structure of each decision tree.
Another interesting point to make is, that random perturbation of an input
variable could in theory be used as part of many other models, including but
not limited to generalized linear models.
Obviously this isn't very useful in the case of generalized linear models, but
the point is, that it can be done.

The way variable selection based on variable importance measures is
being done, is by finding variables whose randomization led to an improvement
in prediction accuracy.
These improvements are just random effects, and function as an indicator
for which variable importances are within the band of random fluctuations,
and which are true indicators for important variables.

\subsubsection{Regression}
Random forest can not only be used for classification, but also for regression.
To produce the numeric output values necessary for regression,
the vote on the most popular output class in the forest is replaced by the
calculation of an average over all tree outputs.
One problem with regression using random forests is that more decision trees
end up covering the middle range of values of output variable.
This means that the prediction accuracy is good in the middle of the range
of values, but that the predictions for extremer values become more and more
inaccurate \cite{zhang2012bias}.
The same paper also suggests five ways to estimate and reduce this bias.

\subsubsection{Overfitting}
Overfitting \cite{wpOF} is the situation where an algorithm learns the
example dataset well enough to not only reproduce the underlying rules,
but to also reproduce the random errors in the dataset.
This is also called \emph{generalization error}, since the algorithm
generalizes errors in the data by deriving rules for them.
Random forests grow optimal decision trees, meaning they try to represent
each bootstrapped sample perfectly.
Except for the case where different records in the sample have the same
values in their input variables, but different values in their output variable,
this leads to decision trees which represent the sample perfectly,
including all errors.
The advantage of ensemble learning is that by combining decision trees grown
for different samples, and constructed using different random split selections,
these errors are canceled out.
However, this also means that there is an upper bound on how accurate
random forests can become depending on the noise present in the dataset.
Of course, this only applies if the trees in an ensemble are reasonably
diverse, i.e. uncorrelated \cite{breiman2001random}.
An analysis by \cite{liu2005maximizing} showed that the generalization error
is at its lowest when the tree ensemble is maximally diverse,
and that bootstrapping tends to limit tree diversity.

Different variants of random forests try to grow random forests on data
that is less prone to noise.
FRF stands for \emph{Fuzzy random forests}, and is a method described by
\cite{bonissone2008fuzzy}.
FRF use what are called \emph{fuzzy sets} to represent the data in the dataset,
and to build their decision trees on top.
The advantages of this approach are that the resulting FRF are more immune
to noise, and \cite{cadenas2012extending} extend this framework to
handle missing data too.
Another attempt at constructing random forests on higher quality data are
rotation forests by \cite{rodriguez2006rotation}, which use
principal component analysis, or PCA, to find derived input variables that
represent the input dimensions better.
Although linear combinations of input dimensions aren't the same thing
as noise, since datasets are of a limited size, and decision trees
might well fail to tease out the interactions between all pairs of input
variables, they will look like noisy input data if the decision trees
don't catch on.
Because better input dimensions lead to better splits, and to more
uncorrelated random variable selections, rotation forests are often more
accurate than random forests \cite{rodriguez2006rotation}.

\subsubsection{Optimality Criterion}
Random forests construct the underlying decision trees by selecting the
best split at each node from a number of randomly selected variables.
To compare the different possible splits, an optimality criterion gets
calculated for each split.
The most commonly used criterion is the Gini index.
The gini index is “a measure of statistical dispersion developed by the
Italian statistician and sociologist Corrado Gini” \cite{wpGINI}.
As has been mentioned in the section on variable importance, the fact that
the Gini index is biased towards variables with many different values
is problematic.
Another common optimality criterion is entropy, although it suffers from
the same bias towards variables with many different values.
The point of using an optimality criterion in the first place is,
to grow trees using an impurity reduction algorithm, i.e. to select the split
which returns the least dispersed subsets is selected recursively at each node.

A huge disadvantage of using an optimality criterion is the computational cost
one has to pay.
Trees who use random splits are much faster to grow, e.g. PERT ensembles
are claimed to be two orders of magnitude faster than the classical
random forests \cite{cutler2001pert}.

\subsection{In Search of the Super-Forest}
The following section will look at some of the random forest variants that
have already been mentioned above. It concludes by calling for a more
user-friendly random forest variant, which combines many of the features of
the proposed variants, and is tailored to the less technically adept users
in the social sciences.

\subsubsection{Alternative: PERT}
PERT, a variant in the random forests framework was developed
by \cite{cutler2001pert}.
The nice thing about PERT, is that its strong reliance on randomness gives
it a simple structure that is easier to describe and implement than the
classical random forests by \cite{breiman2001random}.
In fact, PERT is simple enough that its algorithm is completely described
in the following quote from \cite{cutler2001pert}:

\begin{quotation}
The perfect random tree ensemble, PERT, is so named because the PERT base
learner is a random tree classifier that fits the training data perfectly.
The PERT base learner is constructed by first placing all data points in
the root node.
At each step of the tree construction, each nonterminal node is split by
randomly choosing two data points from the node until these two belong to
different classes.
Let \( x = (x_{1} , . . . , x_{p} ) \) and \( z = (z_{1} , . . . , z_{p} ) \).
If this is not possible, all the data points in the node must be from the
same class and the node is terminal.
Now, randomly choose a feature on which to split, say feature j, and split
at \( \alpha x_{j} + (1 - \alpha) z_{j} \), where \( \alpha \) is generated
from a uniform(0,1) distribution.

Ties are taken care of by repeating the procedure until a definitive
split is obtained.
If no such split is found after 10 tries, the node is declared to be
terminal (so in this case, the tree would not perfectly fit the data).
To form an ensemble, PERT base learners can be combined by simply fitting
many PERT base learners to the entire training set and voting these
classifiers.
Alternatively, PERT base learners can be combined by bagging
\cite{breiman1996bagging}.
In this case, the PERT base learner is fit to many bootstrap samples
from the training data and these classifiers are combined by voting.
\end{quotation}

On the one hand, bagging is not required, because PERT doesn't depend on
randomness introduced through bootstrapping.
On the other hand, bagging is necessary if one requires out-of-bag
error estimates or variable importance measures.
As \cite{liu2005maximizing} have indicated, bootstrapping might lead
to less diverse trees, thus it should not be used unless one requires
these features.

The advantage of PERT over random forests is speed.
\cite{cutler2001pert} compare the different runtimes of classical random
forests construction, as described in \cite{breiman2001random}, and PERT
to find the latter to be faster by two orders of magnitude,
while providing similar accuracy in prediction.
Of course, there are other advantages too:
The absence of a variable selection criterion means that PERT doesn't suffer
from the bias introduced by it, thus, variable importance estimates are less
affected.
An instance where the use of a selection criterion is especially problematic
is the XOR problem mentioned above in the section on variable importance.
Since PERT forests don't depend on an optimality criterion to select their
splits, PERT has the advantage of being able to deal with perfect
interaction effects.
A disadvantage is that PERT forests are even more difficult to interpret than
the classical random forests, but since this is rarely tried anyway,
it probably doesn't matter much in practice.
It is important to mention however, that PERT can only be used for
classification,
since its base learner fits the training data perfectly, applying PERT to
regression would “drastically overfit the data” \cite{cutler2001pert}.

PERT isn't the only random forest variant that emphasizes randomness in its
algorithm for growing trees.
\cite{geurts2006extremely} suggested what they call extremely randomized trees.
Despite their name, extremely randomized trees are actually closer to the
trees of random forests by \cite{breiman2001random} than PERT.
Extremely randomized trees allow the user to configure the randomness of the
constructed decision trees through an additional parameter in order to adapt
the random forest algorithm to the problem domain of the dataset at hand.

\subsubsection{Alternative: Fuzzy random forests}
Fuzzy random forests by \cite{bonissone2008fuzzy} use input variables
encoded as fuzzy sets.
Fuzzy sets \cite{wpFS} model membership to a category by a value between
one and zero, while fuzzy logic \cite{wpFL} allows to reason using fuzzy data:
For example, a day with a temperature of 45 degrees Celsius is to 100\% a
hot day, and to 0\% a cold day, while a day with 15 degrees Celsius might
belong to the label cold day by 40\% and to hot day by 60\%.
The advantage of using fuzzy sets is the added flexibility of representing
uncertainty.
\cite{cadenas2012extending} extended fuzzy random forests to handle
missing data, thus making fuzzy random forests even better at representing
real world data.

\subsubsection{Alternative: Rotation forests}
Rotation forests use principle component analysis to transform
the dataset before creating the ensemble of decision trees.
"Principal component analysis (PCA) is a mathematical procedure that uses
an orthogonal transformation to convert a set of observations of possibly
correlated variables into a set of values of linearly uncorrelated variables
called principal components." \cite{wpPCA}
Rotation forests have been suggested by \cite{rodriguez2006rotation}.
Rotation forests are created by calculating a principal component analysis
on a bootstrapped sample of the data for every tree.
The input variables are split into subsets, and random values of the output
variable are removed for the principal component analysis, which means that
another parameter for the size of the input variable subsets is required.
Rotation forests significantly outperform random forests, bagging, and
boosting on many datasets \cite{rodriguez2006rotation}.

The transformation of the input data leads to less correlation between trees,
and to improved prediction accuracy, which \cite{rodriguez2006rotation}
demonstrate with the help of diversity-error diagrams.

\subsubsection{Alternative: Unknown}
The different algorithms in the random forest framework have been developed
iteratively, with each improvement or variation picking up one problem to
improve on.
The criterion for success has been beating the base algorithm.
Similar to the how the scientific method has become a competition between
researcher and randomness, the field of research on random forests seems
to be plagued by comparisons between the old and the new forests.
It might be worth the effort to step back from the specifics for a moment
and look at the principle behind random forests.

Every variant in the random forests framework is an ensemble of trees.
Beyond that there seems to be nothing that may not be changed.
Three questions that naturally follow from this are:

\begin{APAenumerate}
\item Can a random forest incorporate all of the developed features?
\item How does one prioritize the different features and problems?
\item Is there a different and stronger representation for random forests?
\end{APAenumerate}

Most of the suggested improvements and variants have taken the original
random forest algorithm and changed some part of it. Since the changes of
the different variants discussed in this report don't overlap, combining
them is certainly possible.
The third question is the hardest to answer, and it's only possible to say
that time will tell.
The most interesting question is the second.
This report has mentioned many problems of random forests, for example the
problem of the parameters.
To increase the reach of random forests in general, and particularly in
the social sciences, the parameter situation needs to be resolved, because
not every researcher can include a simulation study just to show that his
methods are not flawed.
By including the suggestions of \cite{banfield2007comparison} regarding
\texttt{ntree} and \cite{bernard2008forest} regarding \texttt{mtry}
a new random forest variant could be completely parameterless.
Alternatively, a new random variant could use PERT to get rid of both, the
parameter \texttt{mtry} and some of the bias in the variable importance
measure.

Obviously, there is no single best solution, but this doesn't make unifying
random forest variants a futile task.
The simpler and safer a method is to use, the wider it will spread, and by
spreading a good method, all of scientific research will benefit.

\newpage
\section{Method}
\subsection{Selection of Papers}
I started my research on random forests by reading the
introductory paper suggested by my supervisor titled
\emph{An introduction to recursive partitioning: Rationale, application
and characteristics of classification and regression trees, bagging and
random forests} \cite{strobl2009introduction}.
I then searched the research databases \emph{PsychARTICLES} and
\emph{PsychINFO} querying for random forest and limiting my results
to the last two years.
I only considered papers that focus on random forests and
dismissed almost all papers where random forests are merely used as a
research method.
I also used \emph{Google Scholar} to look for the more technical
publications outside the field of psychology.
I searched for combinations of \emph{random forest}, \emph{comparison},
\emph{analysis}, and \emph{ensemble}.
I also included keywords seen in interesting titles, like \emph{fuzzy},
\emph{perfect}, \emph{full}, \emph{balanced}, \emph{extremely}, and
\emph{rotation}.
I the looked for some of the referenced publications I found reading the
papers found as mentioned above, and included \cite{strobl2008conditional},
\cite{ho1998random}, \cite{breiman1996bagging}, \cite{freund1995decision},
and \cite{long2010random}.

The final criteria for inclusion were the online availability of a freely
downloadable PDF-file, which thanks to \emph{Google Scholar} often turned
out to be no problem at all, and my decision on the topic of
this report.

A lot of the information in the fields of computer science, artificial
intelligence, machine learning, databases, and especially programming
I acquired through different means in the last ten years.
As I can't remember the original sources, I include the pages
on Wikipedia, which I used to refresh my memories.

\subsection{Aim and Structure of the Paper}
Possible options for this topic were the presentation of
exemplary uses of random
forests, the discussion of strengths and weaknesses, and any of the more
specialized variants.
During my research I had the impression, that there were serious
differences in the understanding of random forests among researchers,
and even among designers of improved variants.
A study comparing different decision tree ensemble techniques also confirmed
this expression by saying that many of the commonly used methods of comparison
weren't robust enough for use with random forests \cite{banfield2007comparison}.

Because of this fuzziness, I decided on a different focus, and to write
a very broad - big picture - introduction to random forests.

\newpage
\section{Discussion}
State of the art? Well... They are not yet properly understood, and
many comparisons and lots of the advantages might be accidential.
Strong dependance on parameters, with no rationally pleasing way to set them.

\subsection{The End}
Bye.

\bibliography{references}

\end{document}

